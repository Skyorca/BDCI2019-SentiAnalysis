工程实现：
1. 一个Embedding层的输入应该是一系列的整数序列，比如一个2D的输入，它的shape值为(samples, indices)，也就是一个
samples行，indeces列的矩阵。每一次的batch训练的输入应该被padded成相同大小（尽管Embedding层有能力处理不定长序列，如果你不指定
数列长度这一参数） dim). 所有的序列中的整数都将被对应的词向量矩阵中对应的列（也就是它的词向量）代替,比如序列[1,2]将被序列
[词向量[1],词向量[2]]代替。这样，输入一个2D张量后，我们可以得到一个3D张量。所以这不是直接相乘的dense层而是一个映射。
2. more improved ways:
2.1 找中文新闻语料进行词向量训练
2.2 boosting
2.3 修改lstm的层结构，用ConvLSTM2D?
2.4 lstm+cnn: 
    放几层lstm堆叠？一层在epoch=1: acc=0.45/val_acc=0.67
    堆两层CNN?
    增加更多尺寸的卷积核？
    















======================================================================================================
理论：
1. https://cloud.tencent.com/developer/article/1031121   model indedxes

2. 关于textCNN结构调参：
    根据文章中的描述，总结如下。感兴趣的同学请仔细阅读原文
    1. 初始化词向量
    使用word2vec和golve都可以，不要使用one-hot vectors
    2. 卷积核的尺寸
    1-10之间，具体情况具体分析，对最终结果影响较大。一般来讲，句子长度越长，卷积核的尺寸越大。
    3. 每种尺寸卷积核的数量
    100-600之间，对模型性能影响较大，需要注意的是增加卷积核的数量会增加训练模型的实践。
    4. 激活函数的选择
    使用relu函数
    5. drop out rate
    0.0-0.5, 当增加卷积核的数量时，可以尝试增加drop out rate，甚至可以大于0.5
    6. 池化的选择
    1-max pooling
    7. 正则项
    正则项对最终模型性能的影响很小

